<!doctype html>
<html lang="en-GB" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-ai-101/LLMs/develop-LLMs">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Develop large language models | Learning Prompt</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://learningpromt.wiki/docs/ai-101/LLMs/develop-LLMs"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Develop large language models | Learning Prompt"><meta data-rh="true" name="description" content="The primary content of this article is derived from the paper A Survey of Large Language Models."><meta data-rh="true" property="og:description" content="The primary content of this article is derived from the paper A Survey of Large Language Models."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://learningpromt.wiki/docs/ai-101/LLMs/develop-LLMs"><link data-rh="true" rel="alternate" href="https://learningpromt.wiki/docs/ai-101/LLMs/develop-LLMs" hreflang="en-GB"><link data-rh="true" rel="alternate" href="https://learningpromt.wiki/zh-Hans/docs/ai-101/LLMs/develop-LLMs" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://learningpromt.wiki/docs/ai-101/LLMs/develop-LLMs" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Learning Prompt RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Learning Prompt Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9QBEXE7W09"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-9QBEXE7W09",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.61258ccd.css">
<link rel="preload" href="/assets/js/runtime~main.fcc20cc1.js" as="script">
<link rel="preload" href="/assets/js/main.03e6b8d6.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#fafbfc;color:#091E42" role="banner"><div class="content_knG7 announcementBarContent_xLdY">‚≠ê If you have any questions, feel free to join our <a target="_blank" href="https://discord.gg/B7Z7wjuUPg">Discord</a>.‚≠ê</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Learning Prompt logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Learning Prompt logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Learning Prompt</b></a><a class="navbar__item navbar__link" href="/docs/welcome">üëã Welcome</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/ai-101">ü§ñ AI 101</a><a class="navbar__item navbar__link" href="/docs/chatgpt-learning-path">üí¨ ChatGPT</a><a class="navbar__item navbar__link" href="/docs/midjourney-learning-path">üñºÔ∏è Midjourney</a><a class="navbar__item navbar__link" href="/blog">üì∞ Changelog</a></div><div class="navbar__items navbar__items--right"><a href="https://jimmywong.bio/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">üòé About Me<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/ai-101/LLMs/develop-LLMs" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en-GB">English</a></li><li><a href="/zh-Hans/docs/ai-101/LLMs/develop-LLMs" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div><a href="https://github.com/thinkingjimmy/Learning-Prompt" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ai-101">ü§ñ AI 101</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/ai-101/LLMs">üìñ Introduction</a><button aria-label="Toggle the collapsible sidebar category &#x27;üìñ Introduction&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai-101/LLMs/what-is-LLMs">What is a Large Language Model?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai-101/LLMs/how-to-calculate-probability">Calculate probability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/ai-101/LLMs/develop-LLMs">Develop large language models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai-101/LLMs/LLMs-disadvantages">Limitations of LLMs</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/ai-101/best-practice">üèóÔ∏è AI Best Practices</a><button aria-label="Toggle the collapsible sidebar category &#x27;üèóÔ∏è AI Best Practices&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/ai-101/LLMs"><span itemprop="name">üìñ Introduction</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Develop large language models</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>What does it take to develop large language models?</h1><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>The primary content of this article is derived from the paper <a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener noreferrer">A Survey of Large Language Models</a>.</p></div></div><p>After understanding the principles of large language models, you may wonder how they are developed and what the key factors are. Recently, there have been numerous articles claiming that even 5G communication is crucial for developing large language models, solely for the sake of generating traffic üòÇ</p><p>In fact, as we have seen from the previous explanation, one of the key aspects of developing large language models is data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-factor-1-data">Key Factor 1: Data<a href="#key-factor-1-data" class="hash-link" aria-label="Direct link to Key Factor 1: Data" title="Direct link to Key Factor 1: Data">‚Äã</a></h2><p>Training data primarily consists of corpora. Nowadays, many language models use the following types of corpora:</p><ul><li>Books: BookCorpus, which has been commonly used for smaller language models like GPT-2, includes over 11,000 e-books. It mainly comprises novels and biographies, with the most recent update dating back to December 2015. A larger book corpus is Gutenberg, consisting of 70,000 books, including works of fiction, essays, and dramas. It is one of the largest open-source book corpora and was most recently updated in December 2021.</li><li>CommonCrawl: This is currently the largest open-source web crawling database. However, it contains a significant amount of dirty data, so the commonly used subsets are C4, CC-Stories, CC-News, and RealNews. Additionally, two news corpora, REALNEWS and CC-News, were extracted from CommonCrawl.</li><li>Reddit Links: Reddit can be understood as the foreign version of Baidu Tieba + Zhihu. Currently, there are open-source databases such as OpenWebText and <a href="http://PushShift.io" target="_blank" rel="noopener noreferrer">PushShift.io</a>.</li><li>Wikipedia: Wikipedia is the corpus widely used by large models.</li><li>Code: Generally, this includes open-source public code repositories on GitHub or data from StackOverflow. Google previously released a dataset called BigQuery.</li></ul><p>In practice, training large language models involves using a mixture of different data sources, rather than relying solely on a single corpus. For example, the data used for GPT-3 includes a combination of Books, CommonCrawl, Reddit Links, and Wikipedia.</p><p>From a data perspective, it is important to note that Chinese corpora have a small proportion in these models. ChatGPT is able to respond in Chinese because its training data contains some parallel Chinese-English data, allowing the model to &quot;understand&quot; Chinese-like behavior. Therefore, the more likely scenario is that it first generates content in English and then translates it into Chinese. If you were to ask ChatGPT 3.5, &quot;Please give me some Chinese homophonic puns,&quot; you should be able to understand what I mean.</p><p>Here are the results returned by ChatGPT:</p><div class="language-other codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-other codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Here are some examples of Chinese homophonic puns:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;I have evaporated,&quot; meaning &quot;I am a Vaper&quot; (Vaper sounds similar to &quot;evaporated&quot;).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;We need to go through Sunday,&quot; meaning Tuesday, Wednesday, Thursday, Friday, and Saturday (sounds similar to &quot;we need to go through Sunday&quot;).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;When I see you, my heart becomes colder below zero,&quot; meaning &quot;half becomes cooler&quot; (below zero sounds similar to &quot;half&quot; in Chinese).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;Difficult is the journey to Shu, as difficult as reaching 488&quot; (sounds similar to &quot;Difficult is the journey to Shu&quot; and the number 488).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;The onion girl wants to cry, but she can&#x27;t shed tears because the onion makes me cry&quot; (onion and &quot;want to cry&quot; have similar pronunciations in Chinese).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;The rain is falling, and the old hometown has lush vegetation. I heard that you&#x27;re always deep within me&quot; (sounds similar to &quot;deep within&quot; and &quot;search by my side&quot;).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;Your car is driving so fast that it is &#x27;taxi-ing,&#x27; meaning &#x27;ancestors have been brought out&#x27;&quot; (sounds similar to &quot;taxi-ing&quot; and &quot;ancestors&quot;).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-factor-2-algorithms">Key Factor 2: Algorithms<a href="#key-factor-2-algorithms" class="hash-link" aria-label="Direct link to Key Factor 2: Algorithms" title="Direct link to Key Factor 2: Algorithms">‚Äã</a></h2><p>Once you have the data, you also need algorithms for computation. The most commonly used algorithm libraries for developing large language models are:</p><ol><li>Transformers: This is an open-source Python library built using the Transformer architecture.</li><li>DeepSpeed: It is a deep learning optimization library developed by Microsoft.</li><li>Megatron-LM: This is a deep learning library developed by Nvidia.</li><li>JAX: It is a Python library developed by Google for high-performance machine learning algorithms.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-factor-3-computing-power">Key Factor 3: Computing Power<a href="#key-factor-3-computing-power" class="hash-link" aria-label="Direct link to Key Factor 3: Computing Power" title="Direct link to Key Factor 3: Computing Power">‚Äã</a></h2><p>Simply put, computing power refers to computational resources, or hardware. OpenAI has not disclosed the amount of computing resources used for training the GPT-3 language model. However, OpenAI&#x27;s CEO hinted at hardware costs exceeding $100 million. If we assume a cost of $1,000 per GPU, it would have involved around 100,000 GPUs, based on 32-bit computing. This would provide over 100 PFLOPS of computing power, which is approximately a quarter of the computing power of Alibaba Cloud&#x27;s largest data center.</p><p>Note that this estimate is for the time when GPT-3 was trained.</p><p>Furthermore, I would like to share a viewpoint: Do not assume that computing power will always advance to overcome limitations. Computing power will always be a constraint because our demands for artificial intelligence will continuously increase.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-08-10T14:32:23.000Z">Aug 10, 2023</time></b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/ai-101/LLMs/how-to-calculate-probability"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Calculate probability</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/ai-101/LLMs/LLMs-disadvantages"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Limitations of LLMs</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_jeP5 thin-scrollbar theme-doc-toc-desktop"><div class="margin--md"></div><h3 class="padding-left--md padding-top--md margin-bottom--none" style="text-transform:uppercase;font-size:0.75em;color:var(--ifm-color-emphasis-700);letter-spacing:0.5px">Table of Contents</h3><ul class="table-of-contents table-of-contents__left-border"><li><a href="#key-factor-1-data" class="table-of-contents__link toc-highlight">Key Factor 1: Data</a></li><li><a href="#key-factor-2-algorithms" class="table-of-contents__link toc-highlight">Key Factor 2: Algorithms</a></li><li><a href="#key-factor-3-computing-power" class="table-of-contents__link toc-highlight">Key Factor 3: Computing Power</a></li></ul></div></div></div></div></main></div></div></div>
<script src="/assets/js/runtime~main.fcc20cc1.js"></script>
<script src="/assets/js/main.03e6b8d6.js"></script>
</body>
</html>